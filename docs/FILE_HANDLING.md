# ğŸ“ File Handling & Backup Strategy

## ğŸ¯ Quick Answer

**Q: Does scraping create new files or overwrite existing ones?**

**A: It OVERWRITES the same files, BUT now with automatic backups!** âœ…

---

## ğŸ“Š How It Works Now

### **Every Time You Run `update_jobs.py`:**

```
STEP 0: Backup (NEW!)
â”œâ”€ Copies: data/myjobmag_jobs.csv â†’ data/backups/myjobmag_jobs_20241220_214500.csv
â”œâ”€ Copies: data/myjobmag_jobs.json â†’ data/backups/myjobmag_jobs_20241220_214500.json
â””â”€ Copies: data/job_demand_metrics.csv â†’ data/backups/job_demand_metrics_20241220_214500.csv

STEP 1: Scrape Fresh Data
â”œâ”€ Scrapes MyJobMag
â””â”€ OVERWRITES: data/myjobmag_jobs.csv (with new jobs)
â””â”€ OVERWRITES: data/myjobmag_jobs.json (with new jobs)

STEP 2: Update Metrics
â””â”€ OVERWRITES: data/job_demand_metrics.csv (with new metrics)

STEP 3: Cleanup
â””â”€ Keeps last 10 backups, deletes older ones
```

---

## ğŸ What You Get

### **Active Files (Always Latest)**
```
data/
â”œâ”€â”€ myjobmag_jobs.csv         â† Latest scraped jobs (OVERWRITES)
â”œâ”€â”€ myjobmag_jobs.json        â† Latest scraped jobs (OVERWRITES)
â””â”€â”€ job_demand_metrics.csv    â† Latest metrics (OVERWRITES)
```
**Your recommender uses these** âœ…

### **Backup Files (Historical Archive)**
```
data/backups/
â”œâ”€â”€ myjobmag_jobs_20241220_140000.csv      â† Morning scrape
â”œâ”€â”€ myjobmag_jobs_20241220_140000.json
â”œâ”€â”€ job_demand_metrics_20241220_140000.csv
â”œâ”€â”€ myjobmag_jobs_20241220_210000.csv      â† Evening scrape
â”œâ”€â”€ myjobmag_jobs_20241220_210000.json
â”œâ”€â”€ job_demand_metrics_20241220_210000.csv
â””â”€â”€ ... (keeps last 10 of each)
```
**For your records/analysis** ğŸ“Š

---

## âœ… Benefits of This Approach

### **1. Always Fresh Data**
- âœ… Recommender uses latest jobs
- âœ… No stale listings
- âœ… No duplicates

### **2. Historical Tracking**
- âœ… Can compare job trends over time
- âœ… Can restore if scraping fails
- âœ… Can analyze market changes

### **3. Automatic Management**
- âœ… Backups happen automatically
- âœ… Old backups auto-deleted (keeps last 10)
- âœ… No manual file management needed

---

## ğŸ“ˆ Example Timeline

```
Day 1 (Dec 20):
â”œâ”€ 2:00 AM scrape â†’ 500 jobs
â”‚  â”œâ”€ Active: data/myjobmag_jobs.csv (500 jobs)
â”‚  â””â”€ Backup: data/backups/myjobmag_jobs_20241220_020000.csv
â”‚
â””â”€ 2:00 PM scrape â†’ 520 jobs (20 new!)
   â”œâ”€ Active: data/myjobmag_jobs.csv (520 jobs) â† OVERWROTE 500
   â””â”€ Backup: data/backups/myjobmag_jobs_20241220_140000.csv

Day 2 (Dec 21):
â””â”€ 2:00 AM scrape â†’ 510 jobs
   â”œâ”€ Active: data/myjobmag_jobs.csv (510 jobs) â† OVERWROTE 520
   â””â”€ Backup: data/backups/myjobmag_jobs_20241221_020000.csv
```

**Result:**
- Active file always has latest (510 jobs)
- Backups show history (500 â†’ 520 â†’ 510)

---

## ğŸ” Comparing Backups

### **See What Changed Between Scrapes:**

```bash
# Count jobs in current file
python -c "import pandas as pd; print(len(pd.read_csv('data/myjobmag_jobs.csv')))"

# Count jobs in yesterday's backup
python -c "import pandas as pd; print(len(pd.read_csv('data/backups/myjobmag_jobs_20241219_020000.csv')))"

# Compare departments
python -c "
import pandas as pd
current = pd.read_csv('data/myjobmag_jobs.csv')
old = pd.read_csv('data/backups/myjobmag_jobs_20241219_020000.csv')
print('Current:', current['Department'].value_counts())
print('Old:', old['Department'].value_counts())
"
```

---

## ğŸ›¡ï¸ Backup Management

### **Automatic Cleanup**
- Keeps last **10 backups** of each file type
- Automatically deletes older backups
- Saves disk space

### **Manual Backup Management**

**View all backups:**
```bash
ls data/backups/
```

**Delete all backups (if needed):**
```bash
rm data/backups/*
```

**Keep specific backup:**
```bash
# Copy important backup to safe location
cp data/backups/myjobmag_jobs_20241220_020000.csv important_backups/
```

---

## ğŸ”„ Restore from Backup

If a scrape fails or you want to restore old data:

```bash
# Restore from specific backup
cp data/backups/myjobmag_jobs_20241220_020000.csv data/myjobmag_jobs.csv
cp data/backups/myjobmag_jobs_20241220_020000.json data/myjobmag_jobs.json
cp data/backups/job_demand_metrics_20241220_020000.csv data/job_demand_metrics.csv

# Restart Streamlit
streamlit run app.py
```

---

## ğŸ“Š File Naming Convention

```
Format: {filename}_{YYYYMMDD}_{HHMMSS}.{ext}

Examples:
myjobmag_jobs_20241220_140530.csv
                â”‚        â”‚
                â”‚        â””â”€ Time: 2:05:30 PM
                â””â”€ Date: Dec 20, 2024

job_demand_metrics_20241221_020000.csv
                    â”‚        â”‚
                    â”‚        â””â”€ Time: 2:00:00 AM
                    â””â”€ Date: Dec 21, 2024
```

---

## ğŸ’¡ Best Practices

### **1. Regular Scraping**
- âœ… Daily scrapes at 2 AM (automated)
- âœ… Backups accumulate automatically
- âœ… Can track market trends

### **2. Monitor Backups**
```bash
# Check backup folder size
du -sh data/backups/

# Count backups
ls data/backups/ | wc -l
```

### **3. Archive Important Backups**
```bash
# Monthly archive (before cleanup deletes them)
mkdir archives/2024-12/
cp data/backups/myjobmag_jobs_20241231_*.csv archives/2024-12/
```

---

## ğŸ¯ Summary

| Aspect | Behavior |
|--------|----------|
| **Active Files** | Always OVERWRITTEN with latest data |
| **Backups** | Automatically created before each scrape |
| **Backup Retention** | Last 10 backups kept |
| **Recommender** | Uses active files (always fresh) |
| **Historical Analysis** | Use backup files |
| **Disk Space** | Auto-managed (old backups deleted) |

---

## â“ FAQ

**Q: Will I lose old data?**  
A: No! Backups are created automatically before each scrape.

**Q: How many backups are kept?**  
A: Last 10 of each file type (configurable in `update_jobs.py`).

**Q: Can I change the retention period?**  
A: Yes! Edit `update_jobs.py` line: `cleanup_old_backups(keep_last_n=10)` to any number.

**Q: What if I want to keep ALL backups?**  
A: Set `keep_last_n=999999` or comment out the cleanup function.

**Q: Do backups slow down the update?**  
A: No, copying files takes < 1 second.

---

## ğŸ‰ Bottom Line

âœ… **Active files** = Always latest (recommender uses these)  
âœ… **Backups** = Historical archive (for your analysis)  
âœ… **Automatic** = No manual work needed  
âœ… **Safe** = Can always restore if needed  

**You get the best of both worlds: fresh data + historical tracking!** ğŸš€
